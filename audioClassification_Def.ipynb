{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvictor728/Classificacao-choro-de-bebe/blob/main/audioClassification_Def.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqJZaIioXU37"
      },
      "source": [
        "Importar bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVjRx4_UcAxr",
        "outputId": "843ec514-861f-47e7-b648-859762b26b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOtbFs_Bd1vC",
        "outputId": "47ad528c-b524-4059-c087-019bb04cb94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/28.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.32.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io) (0.32.0)\n",
            "Installing collected packages: tensorflow_io\n",
            "Successfully installed tensorflow_io-0.32.0\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya98397WUJ9e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import IPython.display as ipd\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import random\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import librosa\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "default_dir = '/content/drive/MyDrive/UFRN/2023.1/A.M./Dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJdyzJqePGU"
      },
      "source": [
        "Passo dois: leitura e classficação dos áudios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx9Wh2a-eXv7"
      },
      "outputs": [],
      "source": [
        "def load_audio(file_path):\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(audio, axis=-1)\n",
        "    normalized_waveform = waveform / tf.reduce_max(tf.abs(waveform))\n",
        "    return normalized_waveform, sample_rate\n",
        "\n",
        "def load_audio_files(path: str, label:str):\n",
        "\n",
        "    dataset = []\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "\n",
        "    for i, file_path in enumerate(walker):\n",
        "        waveform, sample_rate = load_audio(file_path)\n",
        "        dataset.append((waveform, label))\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-OAoek-YgId",
        "outputId": "c2474216-1568-45e7-a537-53c65543a1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Labels: 5\n",
            "Label Names: ['tired', 'belly_pain', 'hungry', 'discomfort', 'data']\n"
          ]
        }
      ],
      "source": [
        "os.chdir(default_dir)\n",
        "labels = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
        "print(f'Total Labels: {len(labels)}')\n",
        "print(f'Label Names: {labels}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZFkqtaGZr3E",
        "outputId": "328f8b17-12f4-4e48-c096-81e7eba683ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of total examples: 451\n"
          ]
        }
      ],
      "source": [
        "filenames = tf.io.gfile.glob(str(default_dir) + '/*/*')\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSh_Uhrnef98"
      },
      "outputs": [],
      "source": [
        "trainset_belly_pain = load_audio_files('./belly_pain', 'belly_pain')\n",
        "trainset_discomfort = load_audio_files('./discomfort', 'discomfort')\n",
        "trainset_hungry = load_audio_files('./hungry', 'hungry')\n",
        "trainset_tired = load_audio_files('./tired', 'tired')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hKN0JBSl4zS",
        "outputId": "e9094e29-e9c9-4c37-cf3a-a6a83ce16626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-135-3420cdfd3dee>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array([data[0] for data in all_data])  # Recursos (waveform)\n"
          ]
        }
      ],
      "source": [
        "# Concatenar todos os conjuntos em um único conjunto de dados\n",
        "all_data = (\n",
        "        trainset_belly_pain +\n",
        "        trainset_discomfort +\n",
        "        trainset_hungry +\n",
        "        trainset_tired\n",
        ")\n",
        "\n",
        "# Separar os recursos (waveform) e os rótulos (etiquetas)\n",
        "X = np.array([data[0] for data in all_data])  # Recursos (waveform)\n",
        "y = np.array([data[1] for data in all_data])  # Rótulos (etiquetas)\n",
        "X = X.reshape(-1, 1)\n",
        "\n",
        "# Realizar o balanceamento usando RandomOverSampler\n",
        "ros = RandomOverSampler()\n",
        "X_balanced, y_balanced = ros.fit_resample(X, y)\n",
        "\n",
        "trainset_belly_pain = X_balanced[y_balanced == 'belly_pain']\n",
        "trainset_discomfort = X_balanced[y_balanced == 'discomfort']\n",
        "trainset_hungry = X_balanced[y_balanced == 'hungry']\n",
        "trainset_tired = X_balanced[y_balanced == 'tired']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3SBgGjssQXY"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "    frame_length = 255\n",
        "    frame_step = 128\n",
        "\n",
        "    audio_length = tf.shape(waveform)[0]\n",
        "    padding_length = tf.maximum(8000 - audio_length, 0)\n",
        "    zero_padding = tf.zeros([padding_length], dtype=tf.float32)\n",
        "\n",
        "    equal_length_waveform = tf.concat([waveform, zero_padding], 0)\n",
        "\n",
        "    spect = tfio.audio.spectrogram(input=equal_length_waveform, nfft=frame_length, window=frame_length, stride=frame_step)\n",
        "\n",
        "    spectrogram = tf.signal.stft(equal_length_waveform, frame_length=frame_length, frame_step=frame_step)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "    return spectrogram, spect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNlPDhDot4oS"
      },
      "outputs": [],
      "source": [
        "def create_images(dataset, label_dir):\n",
        "    # make directory\n",
        "    test_directory = f'./data/test/{label_dir}/'\n",
        "    train_directory = f'./data/train/{label_dir}/'\n",
        "\n",
        "    os.makedirs(test_directory, mode=0o777, exist_ok=True)\n",
        "    os.makedirs(train_directory, mode=0o777, exist_ok=True)\n",
        "\n",
        "    dataset_length = len(dataset)\n",
        "    split_index = int(dataset_length * 0.2)\n",
        "\n",
        "    for i, data in enumerate(dataset):\n",
        "\n",
        "        waveform = data[0]\n",
        "        spectrogram, spect = get_spectrogram(waveform)\n",
        "\n",
        "        # Split test and train images by 20%\n",
        "        if i < split_index:\n",
        "            plt.imsave(f'./data/test/{label_dir}/spec_img{i}.png', spectrogram.numpy(), cmap='gray')\n",
        "        else:\n",
        "            plt.imsave(f'./data/train/{label_dir}/spec_img{i}.png', spectrogram.numpy(), cmap='gray')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1DfhKAauQAx"
      },
      "outputs": [],
      "source": [
        "create_images(trainset_belly_pain, 'belly_pain')\n",
        "create_images(trainset_discomfort, 'discomfort')\n",
        "create_images(trainset_hungry, 'hungry')\n",
        "create_images(trainset_tired, 'tired')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1vS9OVywHo1",
        "outputId": "e3b9e227-cb36-4d0d-9a13-37c60f2c66a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1224 files belonging to 4 classes.\n",
            "Using 244 files for validation.\n",
            "Found 304 files belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "train_directory = './data/train/'\n",
        "test_directory = './data/test/'\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_directory, labels='inferred', label_mode='int', image_size=(425, 129), seed=123,\n",
        "    validation_split=0.2, subset='validation')\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_directory, labels='inferred', label_mode='int', image_size=(425, 129),\n",
        "    validation_split=None, subset=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJN1Htc0x-sU"
      },
      "outputs": [],
      "source": [
        "# Configuração do modelo\n",
        "num_classes = 4\n",
        "img_height = 425\n",
        "img_width = 129\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBvRathyIqR"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.125\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "metrics = ['accuracy']\n",
        "model.compile(optimizer, loss_fn, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uw1QdOdyQZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2c77a7-ff20-45f1-8a77-8ae082e553c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting:\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 19s 2s/step - loss: 1.3683 - accuracy: 0.3238\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 17s 2s/step - loss: 1.2750 - accuracy: 0.4262\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 16s 2s/step - loss: 1.1824 - accuracy: 0.6352\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7849 - accuracy: 0.7172\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.3047 - accuracy: 0.9180\n"
          ]
        }
      ],
      "source": [
        "# Set the epocks\n",
        "epochs = 5\n",
        "print('\\nFitting:')\n",
        "\n",
        "# Train the model.\n",
        "history = model.fit(train_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOLmXdkylz5"
      },
      "source": [
        "Testando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn3FzxAtynKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5468bf-b348-4f2a-d27b-55361e91e3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Number correct: 29 out of 32\n",
            "Accuracy 0.90625\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "batch_size = 0\n",
        "for batch_num, (X, Y) in enumerate(test_ds):\n",
        "    batch_size = len(Y)\n",
        "    pred = model.predict(X)\n",
        "    for i in range(batch_size):\n",
        "        predicted = np.argmax(pred[i], axis=-1)\n",
        "        actual = Y[i]\n",
        "        #print(f'predicted {predicted}, actual {actual}')\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "    break\n",
        "\n",
        "print(f'Number correct: {correct} out of {batch_size}')\n",
        "print(f'Accuracy {correct / batch_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo árvore de decisão**"
      ],
      "metadata": {
        "id": "xHSgfo7DgzsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(waveform, sample_rate):\n",
        "    # Extrair os recursos do áudio usando a biblioteca librosa\n",
        "    # Exemplo: MFCCs (Mel-frequency cepstral coefficients)\n",
        "    mfccs = librosa.feature.mfcc(y=waveform.numpy(), sr=sample_rate.numpy(), n_mfcc=13)\n",
        "    return np.mean(mfccs.T, axis=0)  # Retornar a média dos MFCCs como representação do áudio"
      ],
      "metadata": {
        "id": "jDPg4Z4gg_OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path):\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(audio, axis=-1)\n",
        "    return waveform, sample_rate\n",
        "\n",
        "def load_audio_files(path: str, label: str):\n",
        "    dataset = []\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "\n",
        "    for i, file_path in enumerate(walker):\n",
        "        waveform, sample_rate = load_audio(file_path)\n",
        "        features = extract_features(waveform, sample_rate)\n",
        "        dataset.append((features, label))\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "pDicPYPwlX1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir os dados em conjuntos de treinamento e teste (80% treinamento, 20% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "K1AOnTClmH05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma instância do classificador de árvore de decisão\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "# Treinar o modelo com os dados de treinamento\n",
        "decision_tree_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "CYmub-lPmPkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar a predição no conjunto de teste\n",
        "y_pred = decision_tree_classifier.predict(X_test)\n",
        "\n",
        "# Calcular a acurácia do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Acurácia do modelo: {accuracy}')\n",
        "\n",
        "# Exibir o relatório de classificação\n",
        "class_names = label_encoder.classes_\n",
        "report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "print('Relatório de Classificação:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "edLlWe8YpEU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o modelo treinado em um arquivo\n",
        "model_filename = 'decision_tree_model.joblib'\n",
        "joblib.dump(decision_tree_classifier, model_filename)"
      ],
      "metadata": {
        "id": "jrbFMknZscE1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}